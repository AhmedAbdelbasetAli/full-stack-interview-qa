Absolutely! These are **classic system design and database modeling questions** that test your **creativity, scalability thinking, and real-world data handling skills**.

Letâ€™s answer each like a **senior software engineer or architect**, using **What â†’ How â†’ Why + Real Project Examples**.

---

### ðŸ”¹ Question 31:  
> **How would you design a URL shortener (like bit.ly) from a database perspective?**

#### âœ… Strong Answer:

---

### ðŸŽ¯ Requirements
- Convert long URL â†’ short key (e.g., `bit.ly/abc123`)
- Redirect in < 100ms
- Scale to billions of URLs
- Support analytics (clicks, referrers)
- Optional: TTL, custom keys, user ownership

---

### âœ… Database Schema (PostgreSQL)

```sql
CREATE TABLE urls (
    id BIGSERIAL PRIMARY KEY,          -- Auto-increment ID
    long_url TEXT NOT NULL,            -- Original URL
    short_key CHAR(7) UNIQUE NOT NULL, -- e.g., 'abc123'
    created_at TIMESTAMPTZ DEFAULT NOW(),
    expires_at TIMESTAMPTZ,            -- Optional TTL
    user_id BIGINT,                    -- Optional owner
    click_count BIGINT DEFAULT 0
);

-- Index for fast lookup
CREATE INDEX idx_short_key ON urls(short_key);

-- For analytics
CREATE TABLE click_events (
    id BIGSERIAL,
    url_id BIGINT,
    clicked_at TIMESTAMPTZ DEFAULT NOW(),
    ip_address INET,
    user_agent TEXT,
    referrer TEXT
);
```

---

### âœ… Key Design Decisions

#### 1. **ID Generation**
- Use `BIGSERIAL` â†’ auto-increment
- Convert `id` to **base-62** (a-z, A-Z, 0-9) â†’ `abc123`
- Why not random? Avoids collisions

```java
String shortKey = Base62.encode(id); // 123456 â†’ "abc123"
```

#### 2. **TTL (Time-to-Live)**
- Add `expires_at` â†’ check on redirect
- Use cron job to archive old URLs

#### 3. **Scalability**
- **Shard by `id` % N** for horizontal scale
- **Cache hot keys** in Redis: `GET short:abc123` â†’ long URL

#### 4. **Analytics**
- Async write to `click_events` (Kafka or batch)
- Donâ€™t block redirect

---

### âœ… Real Project Example:

We built an internal **link tracker**:
- Used **PostgreSQL + Redis cache**
- Base-62 encoding for short keys
- Clicks logged via **Kafka** â†’ analytics DB
- Redirect: **15ms p99**

Also supported **custom keys** (e.g., `company.com/campaign`)

---

### ðŸ’¡ Pro Insight:
> The database is just one part â€” **caching and async logging** make it fast.

---

---

### ðŸ”¹ Question 32:  
> **What is the difference between synchronous and asynchronous replication?**

#### âœ… Strong Answer:

This is a **CAP Theorem trade-off**: **Consistency vs Availability**

---

### âœ… Synchronous (Sync) Replication

- **Wait** for replica to acknowledge write **before confirming success**
- Ensures **strong consistency**
- Higher latency

```text
Client â†’ Primary â†’ Replica (wait) â†’ ACK to client
```

âœ… Pros:
- No data loss if primary fails
- Strong consistency

âŒ Cons:
- Slower (latency = network + replica write)
- If replica down â†’ primary blocks

---

### âœ… Asynchronous (Async) Replication

- **Donâ€™t wait** â€” confirm success immediately
- Replica gets changes later
- Risk of **data loss**

```text
Client â†’ Primary (ACK) â†’ Replica (eventually)
```

âœ… Pros:
- Fast
- High availability

âŒ Cons:
- If primary fails before replica gets data â†’ **data loss**
- Replica may be stale

---

### âœ… When to Use Which?

| Use Case | Replication Type |
|--------|------------------|
| **Financial systems** | âœ… Synchronous (no data loss) |
| **Read replicas for analytics** | âœ… Asynchronous (stale OK) |
| **Global apps** | âœ… Async (sync latency too high) |
| **HA with low RPO** | âœ… Sync within region, Async across regions |

---

### âœ… Real Project Example:

We used:
- **Synchronous** replication within **same AWS region** (PostgreSQL streaming)
- **Asynchronous** to **DR region** (100ms lag)
- During outage: DR promoted â†’ lost 2s of data (acceptable per RPO)

Also used **semi-synchronous** (MySQL) â€” wait for at least one replica.

---

### ðŸ’¡ Pro Insight:
> **Synchronous** = "I promise itâ€™s safe"  
> **Asynchronous** = "Iâ€™ll try my best"

Choose based on **RPO (Recovery Point Objective)**.

---

---

### ðŸ”¹ Question 33:  
> **How do you implement soft-schema or schema-less design in a relational database?**

#### âœ… Strong Answer:

You can have **flexible schemas** even in SQL databases.

---

### âœ… Option 1: **JSONB (PostgreSQL)**

Best for **semi-structured data**.

```sql
CREATE TABLE products (
    id SERIAL PRIMARY KEY,
    name TEXT,
    metadata JSONB  -- { "color": "red", "size": "M", "tags": ["sale", "new"] }
);

-- Index
CREATE INDEX idx_metadata ON products USING GIN(metadata);

-- Query
SELECT * FROM products WHERE metadata @> '{"color": "red"}';
```

âœ… Use for:
- Dynamic product attributes
- User preferences
- Configurations

---

### âœ… Option 2: **EAV (Entity-Attribute-Value)**

```sql
CREATE TABLE product_attributes (
    product_id INT,
    attribute_name VARCHAR(50),  -- 'color', 'weight'
    attribute_value TEXT
);
```

âš ï¸ Use with **extreme caution**:
- Hard to query
- No type safety
- Poor performance

âœ… Only for **highly dynamic** cases (e.g., medical records)

---

### âœ… Option 3: **Separate Table per Tenant (Multi-tenancy)**

```sql
-- tenant_acme_products, tenant_xyz_products
```
- Schema can evolve per tenant
- But complex to manage

---

### âœ… Real Project Example:

We used **JSONB** for a **product catalog**:
- Each product had different attributes
- Used `metadata` to store `{"material": "cotton", "fit": "slim"}`
- Indexed with `GIN` for fast filtering
- Avoided EAV hell

Also used **schema versioning** for API compatibility.

---

### ðŸ’¡ Pro Insight:
> Use **JSONB** when you need flexibility â€” but **keep core data in columns**.

---

---

### ðŸ”¹ Question 34:  
> **What is a self-join? Give a real-world example.**

#### âœ… Strong Answer:

---

### ðŸ§  What Is a Self-Join?

A **self-join** is when a table is joined **to itself** â€” used when the table has a **hierarchical or recursive relationship**.

---

### âœ… Real-World Example: Employee â†’ Manager

```sql
-- Table: employees
| id | name    | manager_id |
|----|---------|------------|
| 1  | Alice   | NULL       |
| 2  | Bob     | 1          |
| 3  | Carol   | 1          |
| 4  | Dave    | 2          |
```

Find **each employee and their managerâ€™s name**:

```sql
SELECT 
    e.name AS employee,
    m.name AS manager
FROM employees e
LEFT JOIN employees m ON e.manager_id = m.id;
```

Result:
```
employee | manager
---------|--------
Bob      | Alice
Carol    | Alice
Dave     | Bob
Alice    | NULL
```

---

### âœ… Other Examples

| Use Case | How |
|--------|-----|
| **Category tree** | `categories(id, name, parent_id)` |
| **Threaded comments** | `comments(id, content, parent_id)` |
| **Bill of Materials** | `parts(id, name, parent_part_id)` |

---

### ðŸ’¡ Pro Tip:
> Use **recursive CTEs** for deep hierarchies (e.g., org chart):

```sql
WITH RECURSIVE org AS (
    SELECT id, name, manager_id, 1 AS level
    FROM employees WHERE manager_id IS NULL
    UNION ALL
    SELECT e.id, e.name, e.manager_id, o.level + 1
    FROM employees e JOIN org o ON e.manager_id = o.id
)
SELECT * FROM org;
```

---

### ðŸ’¡ Pro Insight:
> Self-joins turn **one table into two roles** â€” powerful for **hierarchical data**.

---

---

### ðŸ”¹ Question 35:  
> **How do you handle timezone-aware timestamps in a global application?**

#### âœ… Strong Answer:

This is **critical** for global apps â€” get it wrong, and your data is **garbage**.

---

### âœ… Best Practice: **Store in UTC, Convert on Display**

#### 1. **Store in UTC**
- Always use `TIMESTAMPTZ` (PostgreSQL), `TIMESTAMP WITH TIME ZONE`
- Never use `TIMESTAMP` (no zone)

```sql
INSERT INTO events (event_time) VALUES ('2023-10-01 12:00:00+00');
-- Or let DB use UTC: NOW() AT TIME ZONE 'UTC'
```

#### 2. **Convert on Read**
- App or frontend converts to userâ€™s timezone

```java
// Java
ZonedDateTime utc = event.getEventTime().atZone(ZoneOffset.UTC);
ZonedDateTime local = utc.withZoneSameInstant(userZone);
```

```javascript
// JavaScript
new Date(event.event_time).toLocaleString('en-US', { timeZone: userTimezone })
```

#### 3. **Store User Timezone (Optional)**
```sql
ALTER TABLE users ADD COLUMN timezone TEXT DEFAULT 'UTC';
```

---

### âœ… Why This Works

| Issue | Solved By |
|------|---------|
| Daylight Saving Time | UTC doesnâ€™t change |
| User in different zone | Convert at display |
| Server in different zone | Doesnâ€™t matter â€” UTC is universal |
| Data consistency | All times in same zone |

---

### âœ… Real Project Example:

We had a **global scheduling app**:
- All events stored in `TIMESTAMPTZ`
- Frontend used `Intl.DateTimeFormat` to show local time
- Admin dashboard showed UTC
- No confusion during DST transitions

Also used **`AT TIME ZONE`** in PostgreSQL for reporting:
```sql
SELECT event_time AT TIME ZONE 'America/New_York' FROM events;
```

---

### ðŸ’¡ Pro Insight:
> **"Time is hard" â€” store in UTC, display in local, never store local time.**

---

## âœ… Summary Table

| Topic | Key Takeaway |
|------|--------------|
| **URL Shortener** | Base-62 from auto-increment ID + Redis cache |
| **Sync vs Async Replication** | Sync = consistency; Async = availability |
| **Soft-Schema in SQL** | Use JSONB (PostgreSQL) â€” not EAV |
| **Self-Join** | Join table to itself â€” for hierarchies (e.g., org chart) |
| **Timezone Handling** | Store in UTC, convert on display â€” never store local time |

---

---

### ðŸ”¹ Question 36:  
> **What is a deadlock? How do databases detect and resolve it?**

#### âœ… Strong Answer:

---

### ðŸ§  What Is a Deadlock?

A **deadlock** occurs when **two or more transactions** are **each waiting for a resource** that the other holds â€” creating a **circular dependency**.

```text
Transaction A: holds Row 1, wants Row 2
Transaction B: holds Row 2, wants Row 1
â†’ Both wait forever
```

---

### âœ… How Databases Detect It

Databases use a **wait-for graph**:
- Nodes = transactions
- Edges = waiting for a lock
- If a **cycle** is detected â†’ deadlock!

Detection methods:
- **Timeout-based**: Rollback if wait > N seconds
- **Immediate detection**: Constantly check for cycles (PostgreSQL, SQL Server)

---

### âœ… How Databases Resolve It

- **Choose a "victim"** (usually the one with least progress)
- **Roll back one transaction** with error:
  ```sql
  ERROR: deadlock detected
  DETAIL: Process 123 waits for Row 2; Process 456 waits for Row 1.
  ```
- Other transaction proceeds
- Application should **retry** the rolled-back transaction

---

### âœ… Example

```sql
-- Session A
BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE id = 1;

-- Session B
BEGIN;
UPDATE accounts SET balance = balance - 50 WHERE id = 2;
UPDATE accounts SET balance = balance + 50 WHERE id = 1; -- Waits for A

-- Session A
UPDATE accounts SET balance = balance + 100 WHERE id = 2; -- Waits for B â†’ DEADLOCK!
```

Database kills one â†’ the other completes.

---

### âœ… Real Project Example:

We had a **batch job** updating user balances:
- Deadlocked with **web requests**
- Fixed by:
  - Ordering updates by `user_id` (always low â†’ high)
  - Reduced lock time
  - Added retry logic in app

Also used **`NOWAIT`** in PostgreSQL for non-blocking attempts.

---

### ðŸ’¡ Pro Insight:
> Deadlocks are **inevitable** in high-concurrency systems â€” design your app to **retry gracefully**.

---

---

### ðŸ”¹ Question 37:  
> **How do you design a multi-tenant SaaS database? (Shared DB vs Isolated DB vs Schema-per-Tenant)**

#### âœ… Strong Answer:

This is a **critical architectural decision** with **trade-offs in cost, isolation, and complexity**.

---

### âœ… Three Main Patterns

| Pattern | Description | Pros | Cons |
|--------|-------------|------|------|
| **1. Shared DB, Shared Table** | All tenants in same tables, `tenant_id` column | âœ… Cheap, easy to manage | âŒ Risk of data leakage, harder to customize |
| **2. Shared DB, Schema-per-Tenant** | One schema per tenant | âœ… Good isolation, per-tenant extensions | âŒ Harder backups, max schemas limit |
| **3. Isolated DB per Tenant** | Separate database for each tenant | âœ… Maximum isolation, per-tenant tuning | âŒ Expensive, complex ops |

---

### âœ… When to Use Which?

| Use Case | Recommended Pattern |
|--------|----------------------|
| **Startup, low cost** | âœ… Shared DB, Shared Table |
| **Mid-size SaaS, balance** | âœ… Shared DB, Schema-per-Tenant |
| **Enterprise, high security** | âœ… Isolated DB |
| **Regulated data (HIPAA, GDPR)** | âœ… Isolated or Schema-per-Tenant |

---

### âœ… Real Project Example:

We built a **SaaS HR platform**:
- **Shared DB, Shared Table** for small tenants
- `tenant_id` indexed on all tables
- **Row-Level Security (RLS)** in PostgreSQL to prevent leaks
- For **enterprise clients**: **Isolated DB** with dedicated instance

Also used **feature flags** to enable per-tenant logic.

---

### ðŸ’¡ Pro Insight:
> Start with **shared tables + RLS** â€” it scales well.  
> Only isolate when **compliance or performance demands it**.

---

---

### ðŸ”¹ Question 38:  
> **How do you design a multi-tenant SaaS database? (Shared DB vs Isolated DB vs Schema-per-Tenant)**

Wait â€” this is a duplicate of **Q37**.  
Letâ€™s assume you meant:

> **"What is the impact of using `SELECT *` in production?"**

Letâ€™s answer that.

---

### ðŸ”¹ Question 38:  
> **What is the impact of using `SELECT *` in production?**

#### âœ… Strong Answer:

`SELECT *` is a **performance anti-pattern** in production.

---

### âœ… Negative Impacts

| Issue | Explanation |
|------|-------------|
| **Network Overhead** | Fetches unused columns â†’ more data over wire |
| **Memory Usage** | Larger result sets â†’ higher app memory |
| **Caching Inefficiency** | Cache stores more data than needed |
| **Index Ineffectiveness** | Canâ€™t use covering indexes |
| **Schema Fragility** | App breaks if column order changes |
| **Security Risk** | May expose sensitive fields (e.g., `password_hash`) |

---

### âœ… Example

```sql
-- Bad
SELECT * FROM users WHERE id = 123;
-- Returns: id, name, email, ssn, created_at, updated_at, password_hash...

-- Good
SELECT id, name, email FROM users WHERE id = 123;
```

âœ… Saves 80%+ data transfer.

---

### âœ… Real Project Example:

We had a **dashboard** doing `SELECT * FROM audit_log`:
- 50 columns, many `TEXT`
- 10K rows â†’ 500MB transfer
- Timed out

âœ… Fixed:
```sql
SELECT user_id, action, timestamp FROM audit_log WHERE ...
```
â†’ 500MB â†’ **5MB** â†’ loads in 200ms

Also improved **query plan** (used covering index).

---

### ðŸ’¡ Pro Insight:
> `SELECT *` is fine in **ad-hoc queries** â€” never in **application code**.

---

---

### ðŸ”¹ Question 39:  
> **How do you enforce data integrity without foreign keys (e.g., in sharded or NoSQL systems)?**

#### âœ… Strong Answer:

In **sharded SQL** or **NoSQL**, foreign keys are often **not supported** â€” so integrity must be enforced **elsewhere**.

---

### âœ… Strategies

#### 1. **Application-Level Checks**
- Before inserting `order`, verify `user_id` exists via API call
- Risk: **race conditions**, network failures

#### 2. **Event Sourcing / CQRS**
- Emit `UserCreated` event
- `Order Service` listens and maintains a **local user cache**
- On `createOrder`, checks cache

#### 3. **Service Coordination**
- `Order Service` calls `User Service` to validate user
- Use **circuit breakers**, retries

#### 4. **Eventual Consistency + Repair Jobs**
- Allow temporary inconsistency
- Run nightly job: "Find orders with invalid user_id â†’ flag for review"

#### 5. **Soft References**
- Store `user_email` instead of `user_id`
- Email is immutable â€” safer for distributed systems

---

### âœ… Real Project Example:

We used **event-driven integrity** in a **microservices app**:
- `User Service` published `UserCreated`, `UserDeleted`
- `Order Service` subscribed â†’ kept local `users` table
- On order creation: checked local table
- Used **Debezium** for CDC-based sync

âœ… No FKs, but strong eventual integrity.

---

### ðŸ’¡ Pro Insight:
> In distributed systems, **foreign keys are replaced by events and contracts**.

---

---

### ðŸ”¹ Question 40:  
> **What is a lagging index rebuild? How do you perform it with minimal downtime?**

#### âœ… Strong Answer:

---

### ðŸ§  What Is a Lagging Index Rebuild?

An **index rebuild** that happens **without blocking writes** â€” critical for large tables in production.

Traditional `CREATE INDEX` **locks the table** â†’ downtime.

We want **zero-downtime index creation**.

---

### âœ… Solution: **Concurrent Index Creation**

#### PostgreSQL: `CREATE INDEX CONCURRENTLY`
```sql
CREATE INDEX CONCURRENTLY idx_users_email ON users(email);
```

âœ… How it works:
- Builds index **without blocking DML** (INSERT/UPDATE/DELETE)
- Takes longer
- Can be canceled without corrupting data
- Runs in background

âŒ Limitations:
- Cannot run during schema changes
- May fail â€” then index is "invalid" â†’ must drop and retry

---

### âœ… Other Databases

| Database | Command |
|--------|--------|
| **MySQL (InnoDB)** | `CREATE INDEX ... ALGORITHM=INPLACE, LOCK=NONE` |
| **SQL Server** | `CREATE INDEX ... WITH (ONLINE = ON)` |
| **Oracle** | `CREATE INDEX ... ONLINE` |

---

### âœ… Real Project Example:

We added an index on `orders(user_id)` for a 100M-row table:
- Normal `CREATE INDEX` â†’ 45min lock â†’ not acceptable
- Used `CREATE INDEX CONCURRENTLY` â†’ 90min, **zero downtime**
- Monitored with `pg_stat_progress_create_index`

Also used **online schema change tools** like **pt-online-schema-change** (MySQL).

---

### ðŸ’¡ Pro Insight:
> In production, **never block writes** for index creation.  
> Use **concurrent/index-online** methods â€” theyâ€™re slower but safe.

---

## âœ… Summary Table

| Topic | Key Takeaway |
|------|--------------|
| **Deadlock** | Circular wait â†’ DB kills one; app should retry |
| **Multi-Tenant DB** | Shared (cheap), Schema (balanced), Isolated (secure) |
| **SELECT \*** | Never in production â€” wastes resources |
| **Data Integrity (No FK)** | Use events, service calls, or local caches |
| **Lagging Index Rebuild** | Use `CONCURRENTLY` (PostgreSQL) or `ONLINE` (others) |

---

